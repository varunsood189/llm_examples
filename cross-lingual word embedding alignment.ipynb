{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCycI7VFimcF",
        "outputId": "ec662abc-264c-4775-e9d4-2aedeecd6598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (71.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp310-cp310-linux_x86_64.whl size=4296184 sha256=2d36191ae08f864744c9735516bac5f92039676bdaca0856b3e843857ba2490b\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/a2/00/81db54d3e6a8199b829d58e02cec2ddb20ce3e59fad8d3c92a\n",
            "Successfully built fasttext\n",
            "Installing collected packages: faiss-gpu, pybind11, fasttext\n",
            "Successfully installed faiss-gpu-1.7.2 fasttext-0.9.3 pybind11-2.13.6\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy scipy gensim faiss-gpu fasttext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/fastText.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysjwx4kTkXiP",
        "outputId": "c45b99d6-4c7c-4854-d32a-4e480bbda6b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fastText'...\n",
            "remote: Enumerating objects: 3998, done.\u001b[K\n",
            "remote: Counting objects: 100% (1057/1057), done.\u001b[K\n",
            "remote: Compressing objects: 100% (196/196), done.\u001b[K\n",
            "remote: Total 3998 (delta 922), reused 884 (delta 856), pack-reused 2941 (from 1)\u001b[K\n",
            "Receiving objects: 100% (3998/3998), 8.30 MiB | 13.42 MiB/s, done.\n",
            "Resolving deltas: 100% (2529/2529), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/MUSE.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOz6oCMnrQJK",
        "outputId": "f88c39f4-a932-44c1-bcb2-28924df3c1f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MUSE'...\n",
            "remote: Enumerating objects: 239, done.\u001b[K\n",
            "remote: Total 239 (delta 0), reused 0 (delta 0), pack-reused 239 (from 1)\u001b[K\n",
            "Receiving objects: 100% (239/239), 215.77 KiB | 894.00 KiB/s, done.\n",
            "Resolving deltas: 100% (136/136), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/arrival/dictionaries.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsiErLjzwyE1",
        "outputId": "33dca2b6-9bd1-4dc0-f732-af83bac51db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-07 17:01:19--  https://dl.fbaipublicfiles.com/arrival/dictionaries.tar.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.226.210.111, 13.226.210.78, 13.226.210.15, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.226.210.111|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 62097248 (59M) [application/gzip]\n",
            "Saving to: ‘dictionaries.tar.gz’\n",
            "\n",
            "dictionaries.tar.gz 100%[===================>]  59.22M  35.6MB/s    in 1.7s    \n",
            "\n",
            "2024-10-07 17:01:21 (35.6 MB/s) - ‘dictionaries.tar.gz’ saved [62097248/62097248]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp dictionaries.tar.gz /content/MUSE/data/dictionaries.tar.gz"
      ],
      "metadata": {
        "id": "qRF9ZXEsw0_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvzf /content/MUSE/data/dictionaries.tar.gz  -C /content/MUSE/data"
      ],
      "metadata": {
        "id": "rDal7zEVw-Le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./fastText/download_model.py en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhKq6G6glvja",
        "outputId": "62455ad9-4687-4522-de58-66c41b0d8702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
            " (100.00%) [==================================================>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./fastText/download_model.py hi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1KX8ZAikReQ",
        "outputId": "288cce9c-1fcd-4d04-80eb-a373389d17bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.bin.gz\n",
            " (100.00%) [==================================================>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import gzip\n",
        "# Function to extract a .gz file\n",
        "def extract_gz_file(gz_file_path, output_path):\n",
        "    if not os.path.exists(output_path):\n",
        "        print(f\"Extracting {gz_file_path}...\")\n",
        "        with gzip.open(gz_file_path, 'rb') as f_in:\n",
        "            with open(output_path, 'wb') as f_out:\n",
        "                shutil.copyfileobj(f_in, f_out)\n",
        "        print(f\"Extracted to {output_path}\")\n",
        "    else:\n",
        "        print(f\"{output_path} already exists, skipping extraction.\")"
      ],
      "metadata": {
        "id": "ee5pSfawl6mL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths for the .gz files and their extracted versions\n",
        "gz_file_path_en = \"cc.en.300.bin.gz\"\n",
        "output_path_en = \"cc.en.300.bin\"\n",
        "\n",
        "gz_file_path_hi = \"cc.hi.300.bin.gz\"\n",
        "output_path_hi = \"cc.hi.300.bin\"\n",
        "\n",
        "# Extract the .gz files\n",
        "extract_gz_file(gz_file_path_en, output_path_en)\n",
        "extract_gz_file(gz_file_path_hi, output_path_hi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTdVJIovmB3a",
        "outputId": "9098873b-602b-4bbf-a34a-c764b8bd6bb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cc.en.300.bin already exists, skipping extraction.\n",
            "cc.hi.300.bin already exists, skipping extraction.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "\n",
        "# Load pre-trained FastText models\n",
        "en_model = gensim.models.fasttext.load_facebook_vectors('cc.en.300.bin')\n",
        "hi_model = gensim.models.fasttext.load_facebook_vectors('cc.hi.300.bin')\n"
      ],
      "metadata": {
        "id": "ElTh6t37ioT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def limit_vocab(model, top_n=100000):\n",
        "    # Get top N most frequent words\n",
        "    vocab = list(model.key_to_index.keys())[:top_n]\n",
        "    vectors = model[vocab]\n",
        "    return vocab, vectors\n",
        "\n",
        "en_vocab, en_vectors = limit_vocab(en_model, 100000)\n",
        "hi_vocab, hi_vectors = limit_vocab(hi_model, 100000)\n",
        "# Normalize the embeddings\n",
        "en_vectors = en_vectors / np.linalg.norm(en_vectors, axis=1, keepdims=True)\n",
        "hi_vectors = hi_vectors / np.linalg.norm(hi_vectors, axis=1, keepdims=True)\n"
      ],
      "metadata": {
        "id": "9eCDv3xbiss8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_bilingual_lexicon(path):\n",
        "    word_pairs = []\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            en_word, hi_word = line.strip().split()\n",
        "            word_pairs.append((en_word, hi_word))\n",
        "    return word_pairs\n",
        "\n",
        "# Replace with the path to your MUSE dictionary file\n",
        "bilingual_lexicon = load_bilingual_lexicon('MUSE/data/dictionaries/en-hi.txt')\n"
      ],
      "metadata": {
        "id": "CdmSLd4umHJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.linalg import orthogonal_procrustes\n",
        "\n",
        "# Extract embeddings for aligned words\n",
        "def get_aligned_embeddings(lexicon, en_vocab, en_vectors, hi_vocab, hi_vectors):\n",
        "    en_embed, hi_embed = [], []\n",
        "    en_index = {word: idx for idx, word in enumerate(en_vocab)}\n",
        "    hi_index = {word: idx for idx, word in enumerate(hi_vocab)}\n",
        "\n",
        "    for en_word, hi_word in lexicon:\n",
        "        if en_word in en_index and hi_word in hi_index:\n",
        "            en_embed.append(en_vectors[en_index[en_word]])\n",
        "            hi_embed.append(hi_vectors[hi_index[hi_word]])\n",
        "\n",
        "    return np.array(en_embed), np.array(hi_embed)\n",
        "\n",
        "en_aligned, hi_aligned = get_aligned_embeddings(bilingual_lexicon, en_vocab, en_vectors, hi_vocab, hi_vectors)\n",
        "\n",
        "# Procrustes alignment to find the best orthogonal mapping\n",
        "def procrustes(X, Y):\n",
        "    R, _ = orthogonal_procrustes(X, Y)\n",
        "    return R\n",
        "\n",
        "\n",
        "# Get the rotation matrix\n",
        "R = procrustes(en_aligned, hi_aligned)\n",
        "\n",
        "# Apply the transformation to align English vectors to the Hindi space\n",
        "en_aligned_vectors = en_vectors @ R\n"
      ],
      "metadata": {
        "id": "bPxiu9rQmKO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vYrBx5tUIPlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "\n",
        "def get_translation_accuracy(source_vectors, target_vectors, source_vocab, target_vocab, bilingual_lexicon, top_k=5):\n",
        "    # Create a FAISS index and add the target vectors to it for fast nearest neighbor search\n",
        "    index = faiss.IndexFlatL2(target_vectors.shape[1])\n",
        "    index.add(target_vectors.astype('float32'))\n",
        "\n",
        "    # Perform a search for the nearest neighbors\n",
        "    _, neighbors = index.search(source_vectors.astype('float32'), top_k)\n",
        "\n",
        "    # Initialize dictionary to keep track of top-k accuracy counts\n",
        "    top_k_accuracy = {k: 0 for k in range(1, top_k + 1)}\n",
        "\n",
        "    # Iterate over each word in the source vocabulary and its corresponding translations\n",
        "    for i, (en_word, hi_word) in enumerate(bilingual_lexicon):\n",
        "        # Find the nearest neighbors (indices) in the target vocabulary\n",
        "        nearest_indices = neighbors[i]\n",
        "\n",
        "        # Get the corresponding words for the nearest neighbors\n",
        "        nearest_words = [target_vocab[idx] for idx in nearest_indices]\n",
        "\n",
        "        # Check if the correct translation is among the top-k nearest neighbors\n",
        "        for k in range(1, top_k + 1):\n",
        "            if hi_word in nearest_words[:k]:\n",
        "                top_k_accuracy[k] += 1\n",
        "\n",
        "    # Calculate and print the Precision@k for each k\n",
        "    for k in top_k_accuracy:\n",
        "        top_k_accuracy[k] /= len(bilingual_lexicon)\n",
        "        print(f'Precision@{k}: {top_k_accuracy[k] * 100:.2f}%')\n",
        "\n",
        "# Use the updated function with the aligned vectors and bilingual lexicon\n",
        "get_translation_accuracy(en_aligned_vectors, hi_vectors, en_vocab, hi_vocab, bilingual_lexicon)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztq-50HCmNbb",
        "outputId": "33893ab3-5ae2-46dd-d711-f7ee953c8891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision@1: 0.00%\n",
            "Precision@2: 0.01%\n",
            "Precision@3: 0.01%\n",
            "Precision@4: 0.02%\n",
            "Precision@5: 0.02%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Use the first 20,000 word pairs if available in the bilingual lexicon\n",
        "lexicon_subset = bilingual_lexicon[:20000]\n",
        "en_aligned, hi_aligned = get_aligned_embeddings(lexicon_subset, en_vocab, en_vectors, hi_vocab, hi_vectors)\n",
        "R = procrustes(en_aligned, hi_aligned)\n",
        "en_aligned_vectors = en_vectors @ R\n",
        "\n",
        "print(f'\\nResults for lexicon size 20,000:')\n",
        "get_translation_accuracy(en_aligned_vectors, hi_vectors, en_vocab, hi_vocab, lexicon_subset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5h4egusy0380",
        "outputId": "0fa389a6-3a6a-47f2-9f83-e1ce8b69240a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results for lexicon size 20,000:\n",
            "Precision@1: 0.03%\n",
            "Precision@2: 0.03%\n",
            "Precision@3: 0.03%\n",
            "Precision@4: 0.04%\n",
            "Precision@5: 0.06%\n",
            "CPU times: user 5min 4s, sys: 2.59 s, total: 5min 6s\n",
            "Wall time: 1min 33s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def iterative_procrustes(source_embeddings, target_embeddings, initial_R, source_vocab, target_vocab, iterations=5):\n",
        "    R = initial_R\n",
        "    for i in range(iterations):\n",
        "        # Align the source embeddings\n",
        "        source_aligned = source_embeddings @ R\n",
        "\n",
        "        # Find new word pairs using nearest neighbors\n",
        "        index = faiss.IndexFlatL2(target_embeddings.shape[1])\n",
        "        index.add(target_embeddings.astype('float32'))\n",
        "        _, neighbors = index.search(source_aligned.astype('float32'), 1)\n",
        "\n",
        "        # Create new pairs based on the nearest neighbors\n",
        "        new_pairs = [(source_vocab[i], target_vocab[neighbors[i][0]]) for i in range(len(source_vocab))]\n",
        "\n",
        "        # Use new pairs to refine R\n",
        "        new_src, new_tgt = get_aligned_embeddings(new_pairs, source_vocab, source_embeddings, target_vocab, target_embeddings)\n",
        "\n",
        "        # Re-compute the Procrustes transformation matrix\n",
        "        R = procrustes(new_src, new_tgt)\n",
        "\n",
        "    return R\n"
      ],
      "metadata": {
        "id": "wbMe9dN82Zwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dv39CR3259Sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gLHl0cwTzhZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7cuEGTLfI6OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Assuming source_vocab and target_vocab are defined earlier in your code\n",
        "initial_R = procrustes(en_aligned, hi_aligned)\n",
        "\n",
        "# Run iterative refinement with vocabularies\n",
        "refined_R = iterative_procrustes(en_vectors, hi_vectors, initial_R, en_vocab, hi_vocab, iterations=5)\n",
        "\n",
        "# Apply the refined transformation\n",
        "en_aligned_vectors = en_vectors @ refined_R\n",
        "\n",
        "# Re-evaluate accuracy\n",
        "print(f'\\nResults for refined Procrustes:')\n",
        "get_translation_accuracy(en_aligned_vectors, hi_vectors, en_vocab, hi_vocab, lexicon_subset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8lcICEe1LJx",
        "outputId": "cc6357e1-a51f-4c33-ead5-67065e9fa4c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results for refined Procrustes:\n",
            "Precision@1: 0.00%\n",
            "Precision@2: 0.03%\n",
            "Precision@3: 0.03%\n",
            "Precision@4: 0.03%\n",
            "Precision@5: 0.03%\n",
            "CPU times: user 30min 16s, sys: 16.5 s, total: 30min 32s\n",
            "Wall time: 9min 23s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def compute_csls(source_vectors, target_vectors, k=5):\n",
        "    # Compute cosine similarities\n",
        "    sim_matrix = cosine_similarity(source_vectors, target_vectors)\n",
        "\n",
        "    # Get the top k nearest neighbors\n",
        "    knn_source = np.argsort(sim_matrix, axis=1)[:, -k:]\n",
        "    knn_target = np.argsort(sim_matrix, axis=0)[-k:, :]\n",
        "\n",
        "    # Compute CSLS scores\n",
        "    csls_scores = np.zeros(sim_matrix.shape)\n",
        "\n",
        "    for i in range(sim_matrix.shape[0]):\n",
        "        for j in range(sim_matrix.shape[1]):\n",
        "            sim_xy = sim_matrix[i, j]\n",
        "            avg_neigh_source = np.mean(sim_matrix[i, knn_target[:, j]])\n",
        "            avg_neigh_target = np.mean(sim_matrix[knn_source[i], j])\n",
        "            csls_scores[i, j] = 2 * sim_xy - avg_neigh_source - avg_neigh_target\n",
        "\n",
        "    return csls_scores\n"
      ],
      "metadata": {
        "id": "5GB4DgrjI_Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_translation(csls_scores, bilingual_lexicon, top_k=5):\n",
        "    top_k_accuracy = {k: 0 for k in range(1, top_k + 1)}\n",
        "\n",
        "    for i, (source_word, target_word) in enumerate(bilingual_lexicon):\n",
        "        best_indices = np.argsort(-csls_scores[i])[:top_k]\n",
        "        best_words = [target_vocab[idx] for idx in best_indices]\n",
        "\n",
        "        for k in range(1, top_k + 1):\n",
        "            if target_word in best_words[:k]:\n",
        "                top_k_accuracy[k] += 1\n",
        "\n",
        "    for k in top_k_accuracy:\n",
        "        top_k_accuracy[k] /= len(bilingual_lexicon)\n",
        "        print(f'CSLS Precision@{k}: {top_k_accuracy[k] * 100:.2f}%')\n",
        "\n",
        "# Example of usage\n",
        "csls_scores = compute_csls(en_vectors, hi_vectors)\n",
        "evaluate_translation(csls_scores, bilingual_lexicon)\n"
      ],
      "metadata": {
        "id": "j5y73BNp452m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vDf8Ot0P46EG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}